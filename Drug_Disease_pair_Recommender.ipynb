{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z196NsF0LVY7"
   },
   "outputs": [],
   "source": [
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qpl0ndimm5J4"
   },
   "outputs": [],
   "source": [
    "#### PART 2 and 3 of assignment:\n",
    "\n",
    "### both notebook are built with colab cause I didn't have GPU on my local machine\n",
    "#### Since the problem, seem bipartite to me, I took\n",
    "#### this approach instead of treating it as heterogenous graphs\n",
    "#### also specific in the problem set\n",
    "#### the interest was to find score/probability of drug-disease interacting\n",
    "####\n",
    "#### I have added in function to get recomendation of drugs when given a disease\n",
    "#### I have trained model as a classifier with binary entropy loss as a metric with L2 regularization loss\n",
    "#### Since I read a paper on LightGCN hadn't got an opportunity to implemented,\n",
    "#### in bipartite graphs(*https://arxiv.org/abs/2002.02126) I choose this one,\n",
    "#### Though it would have been better if the y variable was a score instead of 1 and 0 classes\n",
    "\n",
    "\n",
    "#### I have only used one archtecture. With more time we can implement graphsage and gcn too\n",
    "#### where we can train different models on different\n",
    "#### seeds(It would take more training time, depending on data size, number of models and epochs etc. )\n",
    "#### the end result would be taken as average or probability from all models or voting from different models for particular interaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pRv9RIJ4HCmA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V4a5vQu2Oclx"
   },
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, disease_embeddings, drug_embeddings, num_layers):\n",
    "        super(LightGCN, self).__init__()\n",
    "\n",
    "        # Convert numpy arrays to torch tensors if they aren't already\n",
    "        if not isinstance(disease_embeddings, torch.Tensor):\n",
    "            disease_embeddings = torch.FloatTensor(disease_embeddings)\n",
    "        if not isinstance(drug_embeddings, torch.Tensor):\n",
    "            drug_embeddings = torch.FloatTensor(drug_embeddings)\n",
    "\n",
    "        # Create embedding layers and register as parameters\n",
    "        self.num_diseases = disease_embeddings.shape[0]\n",
    "        self.num_drugs = drug_embeddings.shape[0]\n",
    "        self.embedding_dim = disease_embeddings.shape[1]\n",
    "\n",
    "        # Register embeddings as parameters\n",
    "        self.disease_embedding = nn.Parameter(disease_embeddings)\n",
    "        self.drug_embedding = nn.Parameter(drug_embeddings)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        # Get embeddings\n",
    "        diseases_emb = self.disease_embedding\n",
    "        drugs_emb = self.drug_embedding\n",
    "        all_emb = torch.cat([diseases_emb, drugs_emb])\n",
    "\n",
    "        # Storage for embeddings at each layer\n",
    "        embs = [all_emb]\n",
    "\n",
    "        # Compute adjacency matrix\n",
    "        adj = torch.zeros((all_emb.shape[0], all_emb.shape[0]), device=edge_index.device)\n",
    "        adj[edge_index[0], edge_index[1]] = 1\n",
    "        adj[edge_index[1], edge_index[0]] = 1\n",
    "\n",
    "        # Compute degree matrix\n",
    "        degree = adj.sum(dim=1)\n",
    "        degree_sqrt = torch.sqrt(degree + 1e-12)\n",
    "        degree_matrix_inv_sqrt = torch.diag(1.0 / degree_sqrt)\n",
    "\n",
    "        # Normalize adjacency matrix\n",
    "        norm_adj = degree_matrix_inv_sqrt @ adj @ degree_matrix_inv_sqrt\n",
    "\n",
    "        # Message passing layers\n",
    "        for _ in range(self.num_layers):\n",
    "            all_emb = norm_adj @ all_emb\n",
    "            embs.append(all_emb)\n",
    "\n",
    "        # Final embeddings are mean of all layers\n",
    "        final_embs = torch.stack(embs, dim=0).mean(dim=0)\n",
    "\n",
    "        diseases_emb_final, drugs_emb_final = torch.split(final_embs, [self.num_diseases, self.num_drugs])\n",
    "        return diseases_emb_final, drugs_emb_final\n",
    "\n",
    "    def predict(self, disease_indices, drug_indices, edge_index):\n",
    "        diseases_emb_final, drugs_emb_final = self.forward(edge_index)\n",
    "        disease_emb = diseases_emb_final[disease_indices]\n",
    "        drug_emb = drugs_emb_final[drug_indices]\n",
    "\n",
    "        # Compute prediction scores\n",
    "        predictions = (disease_emb * drug_emb).sum(dim=1)\n",
    "        return torch.sigmoid(predictions)\n",
    "\n",
    "def prepare_data(ground_truth, node_embeddings, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare train and test datasets using the ground truth data and initial embeddings\n",
    "    \"\"\"\n",
    "    # Create mapping dictionaries for diseases and drugs\n",
    "    disease_to_idx = {disease: idx for idx, disease in enumerate(ground_truth['target'].unique())}\n",
    "    drug_to_idx = {drug: idx for idx, drug in enumerate(ground_truth['source'].unique())}\n",
    "\n",
    "    # Extract embeddings from node_embeddings DataFrame\n",
    "    drug_embeddings = []\n",
    "    disease_embeddings = []\n",
    "\n",
    "    # Get the embedding columns\n",
    "    embedding_cols = [col for col in node_embeddings.columns if col.startswith('embedding_')]\n",
    "\n",
    "    # Create embeddings matrices\n",
    "    for drug in drug_to_idx:\n",
    "        drug_embedding = node_embeddings[node_embeddings['id'] == drug][embedding_cols].values[0]\n",
    "        drug_embeddings.append(drug_embedding)\n",
    "\n",
    "    for disease in disease_to_idx:\n",
    "        disease_embedding = node_embeddings[node_embeddings['id'] == disease][embedding_cols].values[0]\n",
    "        disease_embeddings.append(disease_embedding)\n",
    "\n",
    "    drug_embeddings = np.stack(drug_embeddings)\n",
    "    disease_embeddings = np.stack(disease_embeddings)\n",
    "\n",
    "    # Split the data into train and test\n",
    "    train_df, test_df = train_test_split(ground_truth, test_size=test_size,\n",
    "                                        random_state=random_state, stratify=ground_truth['y'])\n",
    "\n",
    "    # Create edge indices for training (using only positive edges)\n",
    "    train_edge_index = []\n",
    "    for _, row in train_df[train_df['y'] == 1].iterrows():\n",
    "        drug_idx = drug_to_idx[row['source']]\n",
    "        disease_idx = disease_to_idx[row['target']]\n",
    "        train_edge_index.append([drug_idx, disease_idx + len(drug_to_idx)])\n",
    "        train_edge_index.append([disease_idx + len(drug_to_idx), drug_idx])\n",
    "\n",
    "    train_edge_index = torch.tensor(train_edge_index, dtype=torch.long).t()\n",
    "\n",
    "    # Prepare train pairs and labels (including both positive and negative examples)\n",
    "    train_pairs = []\n",
    "    train_labels = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        drug_idx = drug_to_idx[row['source']]\n",
    "        disease_idx = disease_to_idx[row['target']]\n",
    "        train_pairs.append([disease_idx, drug_idx])\n",
    "        train_labels.append(row['y'])\n",
    "\n",
    "    # Prepare test pairs and labels\n",
    "    test_pairs = []\n",
    "    test_labels = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        drug_idx = drug_to_idx[row['source']]\n",
    "        disease_idx = disease_to_idx[row['target']]\n",
    "        test_pairs.append([disease_idx, drug_idx])\n",
    "        test_labels.append(row['y'])\n",
    "\n",
    "    return {\n",
    "        'train_edge_index': train_edge_index,\n",
    "        'train_pairs': torch.tensor(train_pairs, dtype=torch.long),\n",
    "        'train_labels': torch.tensor(train_labels, dtype=torch.float),\n",
    "        'test_pairs': torch.tensor(test_pairs, dtype=torch.long),\n",
    "        'test_labels': torch.tensor(test_labels, dtype=torch.float),\n",
    "        'disease_to_idx': disease_to_idx,\n",
    "        'drug_to_idx': drug_to_idx,\n",
    "        'disease_embeddings': disease_embeddings,\n",
    "        'drug_embeddings': drug_embeddings\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "\n",
    "#### for Preprocessing Data\n",
    "#### Since the knowledge embedding with pykeen would have taken more time\n",
    "#### I use topology embedding from the bedding file\n",
    "#### edges used were from grouth truth file and only nodes present in ground truth file were used\n",
    "#### after that pre-processing I have saved it in recomendation_pipeline_initial_node_embedding\n",
    "#### After that I use prepare data function to make it in format to use in pyG pipeline\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1WhUpwBaWrPI"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/Ground Truth.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read your data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/Ground Truth.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m node_embeddings \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/recomendation_pipeline_initial_node_embeddings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n",
      "File \u001b[0;32m~/0_REPOS/drug_repurposing/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/0_REPOS/drug_repurposing/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/0_REPOS/drug_repurposing/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/0_REPOS/drug_repurposing/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/0_REPOS/drug_repurposing/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Ground Truth.csv'"
     ]
    }
   ],
   "source": [
    "# Read your data\n",
    "ground_truth = pd.read_csv('/content/Ground Truth.csv')\n",
    "node_embeddings = pd.read_csv('/content/recomendation_pipeline_initial_node_embeddings.csv')\n",
    "\n",
    "# Prepare data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = prepare_data(ground_truth, node_embeddings, test_size=0.2) ###  80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrvze4AXgIOL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lstpHiQHVs7Q",
    "outputId": "7493f047-8050-4630-946d-1c39bc881f39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_edge_index', 'train_pairs', 'train_labels', 'test_pairs', 'test_labels', 'disease_to_idx', 'drug_to_idx', 'disease_embeddings', 'drug_embeddings'])\n",
      "(1554, 128)\n",
      "(2231, 128)\n",
      "1554\n",
      "2231\n"
     ]
    }
   ],
   "source": [
    "### how does prepared data look\n",
    "print(data.keys())\n",
    "### checking dimensions\n",
    "\n",
    "print(data['disease_embeddings'].shape)\n",
    "print(data['drug_embeddings'].shape)\n",
    "print(len(data['disease_to_idx']))\n",
    "print(len(data['drug_to_idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7C0SsPHVsrE",
    "outputId": "9b8c9389-6cbb-4f40-ab85-6dcdbcc6c6b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0s: 13449\n",
      "Number of 1s: 9245\n"
     ]
    }
   ],
   "source": [
    "## just a small check to see if the there is a class imbalance, one class is larger than other but difference is not huge\n",
    "\n",
    "zeros = np.count_nonzero(data['train_labels'] == 0)\n",
    "ones = np.count_nonzero(data['train_labels'] == 1)\n",
    "\n",
    "print(f\"Number of 0s: {zeros}\")\n",
    "print(f\"Number of 1s: {ones}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N62O8EgyVsnF",
    "outputId": "745d94e5-2a9b-4826-8c00-ab1d13baa5c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0s: 3362\n",
      "Number of 1s: 2312\n"
     ]
    }
   ],
   "source": [
    "## same with checking test set\n",
    "\n",
    "zeros = np.count_nonzero(data['test_labels'] == 0)\n",
    "ones = np.count_nonzero(data['test_labels'] == 1)\n",
    "\n",
    "print(f\"Number of 0s: {zeros}\")\n",
    "print(f\"Number of 1s: {ones}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bosHynYXTSoE"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, train_edge_index, train_pairs, train_labels, optimizer, batch_size, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = (len(train_pairs) + batch_size - 1) // batch_size\n",
    "\n",
    "    # Shuffle training data\n",
    "    indices = torch.randperm(len(train_pairs))\n",
    "    train_pairs = train_pairs[indices]\n",
    "    train_labels = train_labels[indices]\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(train_pairs))\n",
    "\n",
    "        batch_pairs = train_pairs[start_idx:end_idx].to(device)\n",
    "        batch_labels = train_labels[start_idx:end_idx].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model.predict(\n",
    "            batch_pairs[:, 0],\n",
    "            batch_pairs[:, 1],\n",
    "            train_edge_index.to(device)\n",
    "        )\n",
    "\n",
    "        # Binary cross entropy loss\n",
    "        loss = F.binary_cross_entropy(predictions, batch_labels)\n",
    "\n",
    "        # Add L2 regularization\n",
    "        l2_reg = 0\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param, p=2)\n",
    "        loss += 0.0001 * l2_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate(model, train_edge_index, test_pairs, test_labels, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = model.predict(\n",
    "            test_pairs[:, 0].to(device),\n",
    "            test_pairs[:, 1].to(device),\n",
    "            train_edge_index.to(device)\n",
    "        )\n",
    "\n",
    "        # Calculate metrics\n",
    "        auc_roc = roc_auc_score(test_labels.cpu(), test_predictions.cpu())\n",
    "        precision, recall, _ = precision_recall_curve(test_labels.cpu(), test_predictions.cpu())\n",
    "        auc_pr = auc(recall, precision)\n",
    "\n",
    "        return {\n",
    "            'AUC-ROC': auc_roc,\n",
    "            'AUC-PR': auc_pr\n",
    "        }\n",
    "\n",
    "def train_and_evaluate(model, data, num_epochs=100, batch_size=128, lr=0.001, device='cuda'):\n",
    "    \"\"\"Train and evaluate the model\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            data['train_edge_index'],\n",
    "            data['train_pairs'],\n",
    "            data['train_labels'],\n",
    "            optimizer,\n",
    "            batch_size,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # Evaluate\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            metrics = evaluate(\n",
    "                model,\n",
    "                data['train_edge_index'],\n",
    "                data['test_pairs'],\n",
    "                data['test_labels'],\n",
    "                device\n",
    "            )\n",
    "            print(f'Epoch {epoch+1}: Loss = {train_loss:.4f}, '\n",
    "                  f'Test AUC-ROC = {metrics[\"AUC-ROC\"]:.4f}, '\n",
    "                  f'Test AUC-PR = {metrics[\"AUC-PR\"]:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p712KbwwVsjK"
   },
   "outputs": [],
   "source": [
    "\n",
    "#######\n",
    "\n",
    "### I was initially thinking of this approach but I think this would work well when y is a interaction score instead of 1/0\n",
    "\n",
    "#######\n",
    "\n",
    "\n",
    "# def bpr_loss(disease_emb_final, disease_emb, pos_drug_emb_final, pos_drug_emb,\n",
    "#              neg_drug_emb_final, neg_drug_emb, lambda_reg=0.001):\n",
    "#     \"\"\"\n",
    "#     Compute BPR loss with regularization\n",
    "#     \"\"\"\n",
    "#     # Regularization loss\n",
    "#     reg_loss = lambda_reg * (disease_emb.norm(2).pow(2) +\n",
    "#                             pos_drug_emb.norm(2).pow(2) +\n",
    "#                             neg_drug_emb.norm(2).pow(2))\n",
    "\n",
    "#     # BPR loss\n",
    "#     pos_scores = torch.mul(disease_emb_final, pos_drug_emb_final).sum(dim=-1)\n",
    "#     neg_scores = torch.mul(disease_emb_final, neg_drug_emb_final).sum(dim=-1)\n",
    "\n",
    "#     bpr_loss = torch.mean(torch.nn.functional.softplus(-(pos_scores - neg_scores)))\n",
    "\n",
    "#     return bpr_loss + reg_loss\n",
    "\n",
    "# def train_epoch_bpr(model, train_edge_index, train_pairs, train_labels, optimizer, batch_size, device):\n",
    "#     \"\"\"Train for one epoch using BPR loss with existing negative edges\"\"\"\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     num_batches = (len(train_pairs) + batch_size - 1) // batch_size\n",
    "\n",
    "#     # Get positive and negative pairs\n",
    "#     pos_mask = train_labels == 1\n",
    "#     neg_mask = train_labels == 0\n",
    "#     pos_pairs = train_pairs[pos_mask]\n",
    "#     neg_pairs = train_pairs[neg_mask]\n",
    "\n",
    "#     # Ensure equal number of positive and negative pairs per batch\n",
    "#     min_pairs = min(len(pos_pairs), len(neg_pairs))\n",
    "#     pos_pairs = pos_pairs[:min_pairs]\n",
    "#     neg_pairs = neg_pairs[:min_pairs]\n",
    "\n",
    "#     # Shuffle both positive and negative pairs\n",
    "#     pos_indices = torch.randperm(len(pos_pairs))\n",
    "#     neg_indices = torch.randperm(len(neg_pairs))\n",
    "#     pos_pairs = pos_pairs[pos_indices]\n",
    "#     neg_pairs = neg_pairs[neg_indices]\n",
    "\n",
    "#     for i in range(0, len(pos_pairs), batch_size):\n",
    "#         batch_end = min(i + batch_size, len(pos_pairs))\n",
    "\n",
    "#         # Get batch of positive and negative pairs\n",
    "#         batch_pos_pairs = pos_pairs[i:batch_end].to(device)\n",
    "#         batch_neg_pairs = neg_pairs[i:batch_end].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Get embeddings\n",
    "#         disease_emb_final, drug_emb_final = model(train_edge_index.to(device))\n",
    "#         disease_emb = model.disease_embedding\n",
    "#         drug_emb = model.drug_embedding\n",
    "\n",
    "#         # Get relevant embeddings for positive and negative samples\n",
    "#         batch_disease_emb_final = disease_emb_final[batch_pos_pairs[:, 0]]\n",
    "#         batch_disease_emb = disease_emb[batch_pos_pairs[:, 0]]\n",
    "\n",
    "#         batch_pos_drug_emb_final = drug_emb_final[batch_pos_pairs[:, 1]]\n",
    "#         batch_pos_drug_emb = drug_emb[batch_pos_pairs[:, 1]]\n",
    "\n",
    "#         batch_neg_drug_emb_final = drug_emb_final[batch_neg_pairs[:, 1]]\n",
    "#         batch_neg_drug_emb = drug_emb[batch_neg_pairs[:, 1]]\n",
    "\n",
    "#         # Calculate BPR loss\n",
    "#         loss = bpr_loss(\n",
    "#             batch_disease_emb_final, batch_disease_emb,\n",
    "#             batch_pos_drug_emb_final, batch_pos_drug_emb,\n",
    "#             batch_neg_drug_emb_final, batch_neg_drug_emb\n",
    "#         )\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / ((len(pos_pairs) + batch_size - 1) // batch_size)\n",
    "\n",
    "# def calculate_recall_at_k(predictions, ground_truth, k_values):\n",
    "#     \"\"\"Calculate Recall@k for multiple k values\"\"\"\n",
    "#     recalls = {}\n",
    "#     predictions_topk = predictions.topk(max(k_values))[1]\n",
    "\n",
    "#     for k in k_values:\n",
    "#         predictions_k = set(predictions_topk[:k].tolist())\n",
    "#         ground_truth_set = set(ground_truth.tolist())\n",
    "#         recall = len(predictions_k.intersection(ground_truth_set)) / len(ground_truth_set)\n",
    "#         recalls[f'recall@{k}'] = recall\n",
    "\n",
    "#     return recalls\n",
    "\n",
    "# def evaluate_with_recall(model, train_edge_index, test_pairs, test_labels, device, k_values=[5, 10, 20]):\n",
    "#     \"\"\"Evaluate the model using Recall@k metrics\"\"\"\n",
    "#     model.eval()\n",
    "#     metrics = {}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         disease_emb_final, drug_emb_final = model(train_edge_index.to(device))\n",
    "\n",
    "#         # Calculate Recall@k\n",
    "#         # Group test data by disease\n",
    "#         test_data = pd.DataFrame({\n",
    "#             'disease_idx': test_pairs[:, 0].cpu().numpy(),\n",
    "#             'drug_idx': test_pairs[:, 1].cpu().numpy(),\n",
    "#             'label': test_labels.cpu().numpy()\n",
    "#         })\n",
    "\n",
    "#         recall_values = {f'recall@{k}': 0.0 for k in k_values}\n",
    "#         num_diseases = 0\n",
    "\n",
    "#         for disease_idx in test_data['disease_idx'].unique():\n",
    "#             # Get positive drugs for this disease in test set\n",
    "#             pos_drugs = test_data[\n",
    "#                 (test_data['disease_idx'] == disease_idx) &\n",
    "#                 (test_data['label'] == 1)\n",
    "#             ]['drug_idx'].values\n",
    "\n",
    "#             if len(pos_drugs) == 0:\n",
    "#                 continue\n",
    "\n",
    "#             # Calculate scores for all drugs\n",
    "#             disease_emb = disease_emb_final[disease_idx].unsqueeze(0)\n",
    "#             scores = torch.mm(disease_emb, drug_emb_final.t()).squeeze()\n",
    "\n",
    "#             # Calculate recall@k\n",
    "#             recalls = calculate_recall_at_k(scores, torch.tensor(pos_drugs), k_values)\n",
    "\n",
    "#             for k, recall in recalls.items():\n",
    "#                 recall_values[k] += recall\n",
    "\n",
    "#             num_diseases += 1\n",
    "\n",
    "#         # Average recall values\n",
    "#         for k in recall_values:\n",
    "#             recall_values[k] /= num_diseases\n",
    "#             metrics[k] = recall_values[k]\n",
    "\n",
    "#     return metrics\n",
    "\n",
    "# def train_and_evaluate(model, data, num_epochs=100, batch_size=128, lr=0.001, device='cuda'):\n",
    "#     \"\"\"Train and evaluate the model using BPR loss and recall metrics\"\"\"\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#     k_values = [5, 10, 20,100]  # Values for recall@k\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Train\n",
    "#         train_loss = train_epoch_bpr(\n",
    "#             model,\n",
    "#             data['train_edge_index'],\n",
    "#             data['train_pairs'],\n",
    "#             data['train_labels'],\n",
    "#             optimizer,\n",
    "#             batch_size,\n",
    "#             device\n",
    "#         )\n",
    "\n",
    "#         # Evaluate\n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             metrics = evaluate_with_recall(\n",
    "#                 model,\n",
    "#                 data['train_edge_index'],\n",
    "#                 data['test_pairs'],\n",
    "#                 data['test_labels'],\n",
    "#                 device,\n",
    "#                 k_values\n",
    "#             )\n",
    "\n",
    "#             print(f'Epoch {epoch+1}: Loss = {train_loss:.4f}')\n",
    "#             for k in k_values:\n",
    "#                 print(f'Test Recall@{k} = {metrics[f\"recall@{k}\"]:.4f}')\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwAZNPVpYiJs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHpYurx8Yh_9",
    "outputId": "2ebd14d5-f9d8-4b90-d2eb-b029a264dfa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 0.4152, Test AUC-ROC = 0.9063, Test AUC-PR = 0.8563\n",
      "Epoch 20: Loss = 0.2788, Test AUC-ROC = 0.9372, Test AUC-PR = 0.9062\n",
      "Epoch 30: Loss = 0.2236, Test AUC-ROC = 0.9451, Test AUC-PR = 0.9189\n",
      "Epoch 40: Loss = 0.1890, Test AUC-ROC = 0.9481, Test AUC-PR = 0.9232\n",
      "Epoch 50: Loss = 0.1622, Test AUC-ROC = 0.9493, Test AUC-PR = 0.9244\n",
      "Epoch 60: Loss = 0.1406, Test AUC-ROC = 0.9494, Test AUC-PR = 0.9233\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize model with intital node embeddings(their features)\n",
    "model = LightGCN(\n",
    "    disease_embeddings=data['disease_embeddings'],\n",
    "    drug_embeddings=data['drug_embeddings'],\n",
    "    num_layers=4\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "train_and_evaluate(model, data, num_epochs=60, batch_size=100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O1wzYcOojzEP"
   },
   "outputs": [],
   "source": [
    "def get_pair_score(model, drug_id, disease_id, data, device):\n",
    "    \"\"\"\n",
    "    Get interaction score for a specific drug-disease pair\n",
    "\n",
    "    Args:\n",
    "        model: Trained LightGCN model\n",
    "        drug_id: Drug identifier (e.g., 'CHEMBL123456')\n",
    "        disease_id: Disease identifier (e.g., 'MONDO:0007186')\n",
    "        data: Dictionary containing model data and mappings\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the score and relevant information\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Check if drug and disease exist in our mappings\n",
    "    if drug_id not in data['drug_to_idx']:\n",
    "        raise ValueError(f\"Drug ID {drug_id} not found in training data\")\n",
    "    if disease_id not in data['disease_to_idx']:\n",
    "        raise ValueError(f\"Disease ID {disease_id} not found in training data\")\n",
    "\n",
    "    # Get indices\n",
    "    drug_idx = data['drug_to_idx'][drug_id]\n",
    "    disease_idx = data['disease_to_idx'][disease_id]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        disease_emb_final, drug_emb_final = model(data['train_edge_index'].to(device))\n",
    "\n",
    "        # Get specific embeddings\n",
    "        disease_emb = disease_emb_final[disease_idx].unsqueeze(0)\n",
    "        drug_emb = drug_emb_final[drug_idx].unsqueeze(0)\n",
    "\n",
    "        # Calculate score\n",
    "        score = torch.mm(disease_emb, drug_emb.t()).squeeze()\n",
    "        probability = torch.sigmoid(score)\n",
    "\n",
    "        return {\n",
    "            'drug_id': drug_id,\n",
    "            'disease_id': disease_id,\n",
    "            'raw_score': score.item(),\n",
    "            'probability': probability.item(),\n",
    "            'disease_emb':list(disease_emb),\n",
    "            'drug_emb':list(drug_emb)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyr2Cmspjy7V",
    "outputId": "ad56da81-3bc4-445a-f9a7-2977de9c77b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for the pair\n",
      "Raw score: 4.3494\n",
      "Probability: 0.9873\n"
     ]
    }
   ],
   "source": [
    "# Get score for a specific pair\n",
    "score_info = get_pair_score(\n",
    "    model=model,\n",
    "    drug_id=\"CHEMBL.COMPOUND:CHEMBL30\",\n",
    "    disease_id=\"MONDO:0007186\",\n",
    "    data=data,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"score for the pair\")\n",
    "print(f\"Raw score: {score_info['raw_score']:.4f}\")\n",
    "print(f\"Probability: {score_info['probability']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F3ulOp1Lj3fD",
    "outputId": "ea4e63cd-5600-4a9d-bc3a-3ebfbdfeea2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drug_id': 'CHEMBL.COMPOUND:CHEMBL30',\n",
       " 'disease_id': 'MONDO:0007186',\n",
       " 'raw_score': 4.349397659301758,\n",
       " 'probability': 0.9872501492500305,\n",
       " 'disease_emb': [tensor([-0.4440, -0.2155,  0.4541, -0.4702,  0.2211,  0.2181, -0.1467,  0.4214,\n",
       "           0.1603, -0.1287,  0.1681,  0.1274, -0.2727,  0.1877,  0.0130, -0.4701,\n",
       "          -0.1617,  0.1998, -0.1888, -0.4296, -0.1751,  0.4310,  0.1423, -0.0080,\n",
       "           0.2045,  0.4471, -0.4535,  0.4224,  0.2488,  0.4404,  0.4259,  0.2253,\n",
       "          -0.2045,  0.4637,  0.1788, -0.4454, -0.1613, -0.2043, -0.1976,  0.4428,\n",
       "          -0.0108, -0.0942, -0.3114, -0.1989,  0.1862,  0.1291, -0.4229,  0.4614,\n",
       "          -0.4687,  0.0755, -0.1765,  0.1390,  0.4331,  0.1520,  0.4555,  0.4051,\n",
       "          -0.1752,  0.3089, -0.2247,  0.1028, -0.1814, -0.0258,  0.4645,  0.2008,\n",
       "           0.4602, -0.1847,  0.2151, -0.1212, -0.4654, -0.4600, -0.1984,  0.0292,\n",
       "           0.1335,  0.1657, -0.4628,  0.1336,  0.0020,  0.1881, -0.1761,  0.3263,\n",
       "           0.4602,  0.4600,  0.1964, -0.1962,  0.4912,  0.2434,  0.4341, -0.4247,\n",
       "          -0.1682,  0.4506,  0.3380,  0.1559, -0.1741, -0.1307,  0.1686, -0.2244,\n",
       "           0.1833, -0.4306, -0.1730,  0.2088, -1.4021, -0.1604, -0.1445,  0.1988,\n",
       "           0.3452,  0.4684,  0.4331,  0.5964, -0.4500, -0.1644, -0.1735,  0.1585,\n",
       "          -0.2138,  0.2041, -0.1669,  0.0871,  0.4558,  0.2048, -0.0278, -0.1543,\n",
       "           0.1849,  0.1726, -0.1858, -0.1618,  0.1804,  0.2043,  0.1340, -0.1969],\n",
       "         device='cuda:0')],\n",
       " 'drug_emb': [tensor([-0.1741, -0.1188,  0.1902, -0.2469,  0.1720,  0.1795, -0.0347,  0.2254,\n",
       "          -0.0631,  0.0180, -0.0744,  0.1013, -0.3135, -0.0975,  0.1077, -0.2653,\n",
       "          -0.2236, -0.1310, -0.1688, -0.1833,  0.0838,  0.1773,  0.2709,  0.1112,\n",
       "           0.0898,  0.1783, -0.3025,  0.2141,  0.3078,  0.1701,  0.2491,  0.1482,\n",
       "          -0.0752,  0.2143,  0.1911, -0.1759, -0.0512, -0.0716, -0.1415,  0.3106,\n",
       "          -0.1478, -0.2491, -0.3202, -0.2927,  0.2882, -0.0220, -0.2395,  0.2927,\n",
       "          -0.2352,  0.2398,  0.0855, -0.0749,  0.1706, -0.0509,  0.1930,  0.3226,\n",
       "          -0.2841,  0.3199, -0.1542, -0.1145,  0.0126, -0.2122,  0.2869,  0.0964,\n",
       "           0.2039, -0.0777, -0.1791, -0.2617, -0.2202, -0.2034,  0.1379, -0.1141,\n",
       "          -0.1326,  0.2234, -0.2903,  0.1036,  0.1098,  0.0815, -0.2844,  0.3220,\n",
       "           0.2041,  0.2034,  0.1459,  0.1400, -0.2477,  0.3064,  0.1690, -0.2030,\n",
       "          -0.2813,  0.1839,  0.3231, -0.0565, -0.1992, -0.0035,  0.2226, -0.1564,\n",
       "          -0.0091, -0.1789, -0.0646, -0.1161,  0.4039, -0.2209,  0.1335,  0.2036,\n",
       "           0.3236,  0.2757,  0.1707, -0.2692, -0.3056,  0.1327,  0.0817, -0.0604,\n",
       "          -0.1867,  0.0676, -0.2098,  0.2456,  0.1937, -0.1101,  0.1139,  0.0552,\n",
       "          -0.0956, -0.0279, -0.1762, -0.2788, -0.1381, -0.1094,  0.1089, -0.0916],\n",
       "         device='cuda:0')]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0b_MfS8Qj3ay"
   },
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "\n",
    "#### This was related to one of the personal projects, (Bipartite graph problem)\n",
    "#### Can we get a list of drugs for a specific disease?\n",
    "####\n",
    "#### the next part is additional caveat for generating recommendation list\n",
    "####\n",
    "#### While in comments, you may see recall@K and bpr as an evaluation metric instead of Roc-auc this was when we would have scorees\n",
    "#### this was because I thought it be better if we would like to use same pipeline for recommendation system when y is a score\n",
    "####\n",
    "\n",
    "\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QLB9-00jyzf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "74bKyOmcVsVC"
   },
   "outputs": [],
   "source": [
    "def generate_recommendations_for_disease(model, disease_id, ground_truth, data, device, top_k=20):\n",
    "    \"\"\"\n",
    "    Generate ranked drug recommendations for a specific disease\n",
    "\n",
    "    Args:\n",
    "        model: Trained LightGCN model\n",
    "        disease_id: Disease identifier (e.g., 'MONDO:0007186')\n",
    "        ground_truth: Original ground truth DataFrame\n",
    "        data: Dictionary containing model data and mappings\n",
    "        device: torch device\n",
    "        top_k: Number of recommendations to return\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with ranked drug recommendations and their scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Get disease index\n",
    "    disease_idx = data['disease_to_idx'][disease_id]\n",
    "\n",
    "    # Get existing positive interactions\n",
    "    existing_interactions = set(\n",
    "        ground_truth[\n",
    "            (ground_truth['target'] == disease_id) &\n",
    "            (ground_truth['y'] == 1)\n",
    "        ]['source'].values\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        disease_emb_final, drug_emb_final = model(data['train_edge_index'].to(device))\n",
    "\n",
    "        # Get disease embedding\n",
    "        disease_emb = disease_emb_final[disease_idx].unsqueeze(0)\n",
    "\n",
    "        # Calculate scores for all drugs\n",
    "        scores = torch.mm(disease_emb, drug_emb_final.t()).squeeze()\n",
    "        # scores = torch.sigmoid(scores)  # Convert to probabilities or raw scores\n",
    "\n",
    "        # Convert to numpy for easier handling\n",
    "        scores = scores.cpu().numpy()\n",
    "\n",
    "        # Create recommendations DataFrame\n",
    "        recommendations = []\n",
    "        for drug_id, drug_idx in data['drug_to_idx'].items():\n",
    "            if drug_id not in existing_interactions:  # Exclude existing positive interactions\n",
    "                recommendations.append({\n",
    "                    'drug_id': drug_id,\n",
    "                    'score': scores[drug_idx],\n",
    "                })\n",
    "\n",
    "        recommendations_df = pd.DataFrame(recommendations)\n",
    "\n",
    "        # Sort by score and get top k\n",
    "        recommendations_df = recommendations_df.sort_values('score', ascending=False).head(top_k)\n",
    "        recommendations_df = recommendations_df.reset_index(drop=True)\n",
    "\n",
    "        # Add rank column\n",
    "        recommendations_df['rank'] = recommendations_df.index + 1\n",
    "\n",
    "        # Reorder columns\n",
    "        recommendations_df = recommendations_df[['rank', 'drug_id', 'score']]\n",
    "\n",
    "    return recommendations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pvcyntgjg51_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0WWRJEEg50H"
   },
   "outputs": [],
   "source": [
    "###############################################################\n",
    "\n",
    "### getting recommendation for drugs for a particular disease #\n",
    "\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8mfDvRmg5tx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJB8NX_vayi5",
    "outputId": "cf59cdbe-fb45-43a0-d843-52822fe7d96a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank                        drug_id      score\n",
      "0      1   CHEMBL.COMPOUND:CHEMBL396778  17.558647\n",
      "1      2  CHEMBL.COMPOUND:CHEMBL1200823  16.249205\n",
      "2      3     CHEMBL.COMPOUND:CHEMBL1351  16.147923\n",
      "3      4  CHEMBL.COMPOUND:CHEMBL2104164  15.868414\n",
      "4      5  CHEMBL.COMPOUND:CHEMBL1569487  15.491830\n",
      "5      6  CHEMBL.COMPOUND:CHEMBL3322001  14.949764\n",
      "6      7   CHEMBL.COMPOUND:CHEMBL279229  14.141775\n",
      "7      8   CHEMBL.COMPOUND:CHEMBL309821  13.904785\n",
      "8      9     CHEMBL.COMPOUND:CHEMBL1078  13.006432\n",
      "9     10   CHEMBL.COMPOUND:CHEMBL589583  12.979447\n",
      "10    11                    CHEBI:36047  12.814536\n",
      "11    12   CHEMBL.COMPOUND:CHEMBL152067  12.802544\n",
      "12    13   CHEMBL.COMPOUND:CHEMBL376488  12.713521\n",
      "13    14   CHEMBL.COMPOUND:CHEMBL203125  12.694095\n",
      "14    15   CHEMBL.COMPOUND:CHEMBL116438  12.666769\n",
      "15    16  CHEMBL.COMPOUND:CHEMBL2110816  12.269520\n",
      "16    17   CHEMBL.COMPOUND:CHEMBL162036  12.225706\n",
      "17    18     CHEMBL.COMPOUND:CHEMBL1072  12.128423\n",
      "18    19                    CHEBI:35457  12.006820\n",
      "19    20  CHEMBL.COMPOUND:CHEMBL1201576  11.988169\n"
     ]
    }
   ],
   "source": [
    "disease_id = 'MONDO:0007186'  # Replace with your disease ID\n",
    "recommendations = generate_recommendations_for_disease(\n",
    "    model=model,\n",
    "    disease_id=disease_id,\n",
    "    ground_truth=ground_truth,\n",
    "    data=data,\n",
    "    device=device,\n",
    "    top_k=20\n",
    ")\n",
    "print(recommendations)\n",
    "\n",
    "\n",
    "####\n",
    "#### This is from one model\n",
    "#### with more models we can can take recommendations as drugs who showed up\n",
    "#### in top 20 or top n and then rank them on importance(frequency of them appearing in the top 20 or top n list)\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkcA5f7IayeE"
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################\n",
    "\n",
    "#### save it and use it later with function ###\n",
    "\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0k10qkZjWjN"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_model_pickle(model, data, save_path='lightgcn_model_2.pkl'):\n",
    "    # Move model to CPU for saving\n",
    "    model = model.cpu()\n",
    "\n",
    "    # Create save dictionary\n",
    "    save_dict = {\n",
    "        'model_state': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'num_diseases': model.num_diseases,\n",
    "            'num_drugs': model.num_drugs,\n",
    "            'embedding_dim': model.embedding_dim,\n",
    "            'num_layers': model.num_layers\n",
    "        },\n",
    "        'mappings': {\n",
    "            'drug_to_idx': data['drug_to_idx'],\n",
    "            'disease_to_idx': data['disease_to_idx']\n",
    "        },\n",
    "        'train_edge_index': data['train_edge_index'].cpu()\n",
    "    }\n",
    "\n",
    "    # Save using pickle\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "def load_model_pickle(load_path='lightgcn_model_2.pkl', device='cuda'):\n",
    "    # Load the pickle file\n",
    "    with open(load_path, 'rb') as f:\n",
    "        save_dict = pickle.load(f)\n",
    "\n",
    "    # Initialize model with saved configuration\n",
    "    model = LightGCN(\n",
    "        disease_embeddings=torch.randn(save_dict['model_config']['num_diseases'],\n",
    "                                     save_dict['model_config']['embedding_dim']),\n",
    "        drug_embeddings=torch.randn(save_dict['model_config']['num_drugs'],\n",
    "                                  save_dict['model_config']['embedding_dim']),\n",
    "        num_layers=save_dict['model_config']['num_layers']\n",
    "    )\n",
    "\n",
    "    # Load model state\n",
    "    model.load_state_dict(save_dict['model_state'])\n",
    "\n",
    "    # Move model to specified device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Prepare data dictionary\n",
    "    data = {\n",
    "        'drug_to_idx': save_dict['mappings']['drug_to_idx'],\n",
    "        'disease_to_idx': save_dict['mappings']['disease_to_idx'],\n",
    "        'train_edge_index': save_dict['train_edge_index'].to(device)\n",
    "    }\n",
    "\n",
    "    return model, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwOjmh32pTEc",
    "outputId": "45dbec58-0800-46b4-9764-0b2b2ec4a156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to lightgcn_model_2.pkl\n"
     ]
    }
   ],
   "source": [
    "# After training your model\n",
    "save_model_pickle(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    save_path='lightgcn_model_2.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e6QvyH13jWfH"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model, data = load_model_pickle(\n",
    "    load_path='lightgcn_model_2.pkl',\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6aGN2C2jWZL",
    "outputId": "6c19d230-68fe-417e-f057-3d44b3d0c98f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for the pair\n",
      "Raw score: 4.3494\n",
      "Probability: 0.9873\n"
     ]
    }
   ],
   "source": [
    "# test that it saved and works alright after load\n",
    "\n",
    "score_info = get_pair_score(\n",
    "    model=model,\n",
    "    drug_id=\"CHEMBL.COMPOUND:CHEMBL30\",\n",
    "    disease_id=\"MONDO:0007186\",\n",
    "    data=data,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"score for the pair\")\n",
    "print(f\"Raw score: {score_info['raw_score']:.4f}\")\n",
    "print(f\"Probability: {score_info['probability']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
